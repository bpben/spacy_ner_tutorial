{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cpmbdch_eLa"
      },
      "source": [
        "# ODSC 2022 NER from scratch with spaCy\n",
        "## Author: Ben Batorsky\n",
        "This notebook contains all the code for the tutorial on NER given at ODSC 2022\n",
        "\n",
        "### Setup\n",
        "If you're running this locally, you should run these cells before the tutorial just to get all set up.\n",
        "\n",
        "If you're running this on Collab, your runtime will be reset after a few minutes of idle time, so you might need to wait until the tutorial starts.  Note that WIFI is spotty in the conference sometimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le7usWlJJzah"
      },
      "outputs": [],
      "source": [
        "# install the required spacy libraries\n",
        "!pip install -q spacy==3.2\n",
        "!pip install -q spacy-transformers==1.1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCHl47rvR8Me",
        "outputId": "f4906169-bb32-4fc1-a846-72bc72ce2ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 45.7 MB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.15)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "# download a more complete model (vectors + NER)\n",
        "!spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Oo9zr3SRYN",
        "outputId": "bd3a4d34-75aa-4f78-f9e9-3a922e118d9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# run this to install the BERT model we'll be using\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "_ = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "_ = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apVRAhh_5mt9",
        "outputId": "6205b7a1-8ab3-4e48-c5ea-7b2128ea4f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'spacy_ner_tutorial' already exists and is not an empty directory.\n",
            "\n",
            "\u001b[38;5;1m✘ Can't clone project, directory already exists: /content/ner_drugs\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# clone the necessary repos\n",
        "# tutorial repo\n",
        "!git clone https://github.com/bpben/spacy_ner_tutorial.git\n",
        "# spacy project for drug NER - using spaCy project CLI, more on this later\n",
        "!spacy project clone tutorials/ner_drugs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQE1jsPlST-d"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPuPr5_3ACjA"
      },
      "source": [
        "## SpaCy basics\n",
        "[Here](https://spacy.io/architecture-415624fc7d149ec03f2736c4aa8b8f3c.svg) is a good overview of how spaCy works.  The next few cells just show you some basic usage and some of the core spaCy types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mJVTdiGABrs",
        "outputId": "eb779632-bb1e-4f5f-ce44-a4b4aef22532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<spacy.tokenizer.Tokenizer object at 0x7f18a1d66f50>\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# import base English language model\n",
        "from spacy.lang.en import English\n",
        "en = English()\n",
        "print(en.tokenizer)\n",
        "# in this base model, there are no pipes (e.g. NER)\n",
        "print(en.pipe_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQL8klj8_XxR",
        "outputId": "575b689a-be2f-4c54-a70a-5a9e54e3aba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "[(We, <class 'spacy.tokens.token.Token'>), (are, <class 'spacy.tokens.token.Token'>), (doing, <class 'spacy.tokens.token.Token'>), (NLP, <class 'spacy.tokens.token.Token'>), (., <class 'spacy.tokens.token.Token'>)]\n"
          ]
        }
      ],
      "source": [
        "# running a \"document\" through a language model\n",
        "text = 'We are doing NLP.'\n",
        "doc = en(text)\n",
        "print(type(doc))\n",
        "print(type(doc[:2]))\n",
        "print([(x, type(x)) for x in doc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHLo2A8dBhyL",
        "outputId": "1122c736-b414-415f-9c83-cb31d3c1c96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "# this language model has a lot more components to it\n",
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNBbT6XABzxV",
        "outputId": "4dcec6b4-fffb-4f14-89f6-1205b71e2b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "()\n",
            "(NLP,)\n"
          ]
        }
      ],
      "source": [
        "# our base model has no entities, our expanded model does\n",
        "doc = en(text)\n",
        "doc_expanded = nlp(text)\n",
        "print(doc.ents)\n",
        "print(doc_expanded.ents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvlHmJs3CBTN",
        "outputId": "0dbfc3ea-45ba-4063-9455-e8e91eef4b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP <class 'spacy.tokens.span.Span'> ORG Companies, agencies, institutions, etc.\n"
          ]
        }
      ],
      "source": [
        "# can get some additional information from these entities\n",
        "# the model thinks NLP is an organization.  \n",
        "# How silly, it'd be great if we could train it to do better...\n",
        "for e in doc_expanded.ents:\n",
        "    print(e, type(e), e.label_, spacy.explain(e.label_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0TURkIEz_Ns",
        "outputId": "0edc819c-e080-4a79-acf1-58bff5c81bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.20593786e-01  1.88496396e-01 -2.94177979e-01 -3.66981983e-01\n",
            "  1.33377999e-01 -1.61488019e-02  5.73446043e-02 -1.06618002e-01\n",
            "  2.76278015e-02  1.93136191e+00 -1.90531999e-01  1.27237201e-01\n",
            "  1.01299003e-01 -3.91399972e-02 -7.14469999e-02 -1.82412818e-01\n",
            " -5.17126098e-02  9.75196004e-01 -3.70818198e-01  1.96072400e-01\n",
            "  1.01875998e-01  9.30623859e-02  1.64807942e-02  3.42839723e-03\n",
            " -1.31522596e-01  1.55715004e-01 -2.13100016e-02 -2.13807389e-01\n",
            "  1.00509003e-01 -7.03402013e-02 -1.69712044e-02 -1.84871599e-01\n",
            "  4.93465960e-02  2.07261011e-01  1.51058003e-01  2.21679598e-01\n",
            "  6.01292029e-02  9.42348018e-02 -9.97439995e-02  1.43356994e-02\n",
            " -1.62757203e-01 -1.84749905e-02  2.60500014e-02  6.06500022e-02\n",
            " -4.75140056e-03  1.25824988e-01 -1.59984201e-01 -9.57759935e-03\n",
            " -1.30816400e-01 -9.82340351e-02 -7.52495974e-02 -1.50035396e-01\n",
            "  6.98117912e-02 -1.49040017e-02  5.82774095e-02 -8.63535982e-03\n",
            " -5.30104041e-02  1.86289959e-02 -1.67561788e-02 -1.16726004e-01\n",
            " -7.27300048e-02 -7.03819990e-02  9.44400392e-03 -4.22129557e-02\n",
            "  8.13963935e-02 -9.60725993e-02  7.52548948e-02  1.92977786e-01\n",
            " -3.61940041e-02  1.39704987e-01 -7.35775605e-02 -7.17997551e-04\n",
            "  5.06410003e-01 -4.66776006e-02  4.49555963e-01  2.36898009e-02\n",
            "  2.04734400e-01 -6.55533969e-02 -1.27662001e-02  2.77668208e-01\n",
            "  5.09400386e-03  8.00618008e-02 -1.36114046e-01  3.50371972e-02\n",
            " -1.72039807e-01 -3.20556581e-01  2.24280000e-01 -3.66340816e-01\n",
            "  2.28575796e-01  1.00198269e-01 -1.28517598e-01  4.31233980e-02\n",
            " -3.96948047e-02  7.60841966e-02  9.28699970e-02  1.37132600e-01\n",
            " -4.34675962e-02 -2.41404533e-01  1.24778030e-02 -2.25173995e-01\n",
            " -9.27193984e-02  1.20945379e-01 -6.37459978e-02 -8.93862024e-02\n",
            "  2.13774592e-01 -8.80393982e-01  1.24364004e-01 -1.56024009e-01\n",
            " -1.31356806e-01  1.87645108e-01 -7.63002038e-02 -3.30357969e-01\n",
            "  1.70837998e-01 -2.51994193e-01  2.30487939e-02  1.76092368e-02\n",
            " -2.35261973e-02  7.12959468e-03  4.29680049e-02 -4.52740043e-02\n",
            "  1.12687409e-01 -1.11529827e-01  1.00477003e-01  2.37056799e-02\n",
            "  9.07135978e-02  1.00127794e-01 -9.17756036e-02 -1.60321593e-01\n",
            "  1.66002005e-01 -2.35726029e-01  8.31819922e-02 -1.18041001e-01\n",
            " -3.83446038e-01  2.45282017e-02  2.39230588e-01  1.07621804e-01\n",
            " -2.09469944e-02 -9.23548117e-02  1.39439806e-01  1.19892001e-01\n",
            " -1.15216196e+00  1.85913414e-01  1.15388200e-01 -3.50680575e-03\n",
            "  1.24668002e-01 -3.88740003e-02 -6.24194033e-02  1.35709196e-01\n",
            " -2.77418010e-02 -1.29846796e-01 -9.02267918e-02 -4.93546017e-02\n",
            "  3.61442827e-02  2.74231993e-02 -1.47851199e-01  3.72389182e-02\n",
            " -6.39188066e-02 -1.34443194e-01 -1.24049999e-01 -1.85327798e-01\n",
            "  1.03159966e-02  1.73651818e-02 -3.39803994e-01 -5.61967976e-02\n",
            "  7.93379992e-02 -1.25266597e-01 -3.19880024e-02 -2.12974221e-01\n",
            "  1.21915996e-01 -1.34720011e-02 -5.60819916e-02  8.03000294e-03\n",
            "  1.18245386e-01 -4.31674011e-02 -1.23711467e-01  3.90259922e-03\n",
            " -1.36007946e-02  1.33086205e-01  1.69829994e-01 -1.92853995e-02\n",
            "  1.57389387e-01  9.86982584e-02 -3.63856018e-01  1.44776050e-02\n",
            "  4.54463959e-02  5.80012784e-05  1.64833009e-01  1.00561202e-01\n",
            "  2.15094209e-01  7.34188110e-02  1.77461997e-01 -7.79604018e-02\n",
            " -1.88950058e-02 -2.64062583e-01 -1.17707208e-01  6.52416050e-02\n",
            "  1.22497201e-01 -1.71227142e-01  3.22728604e-01  3.06222022e-01\n",
            " -1.26495004e-01 -1.97168797e-01  1.57944590e-01  5.76892979e-02\n",
            "  1.19275853e-01 -1.84787184e-01 -9.69211981e-02  1.01589952e-02\n",
            " -4.31777947e-02 -7.97974020e-02  4.04567942e-02  1.53813004e-01\n",
            "  1.02519795e-01  6.91499840e-03 -8.96119997e-02  1.39196040e-02\n",
            " -3.84610072e-02  5.91589920e-02 -7.91768059e-02  1.45722196e-01\n",
            " -7.01143965e-02  1.07993208e-01 -1.42708030e-02 -6.22766092e-02\n",
            " -8.32662135e-02  5.97357042e-02 -8.98719952e-02 -5.14567979e-02\n",
            " -1.57165989e-01 -2.53830016e-01 -1.36767998e-01 -6.07115999e-02\n",
            "  2.42068008e-01  4.33004014e-02  1.18566394e-01 -2.22429968e-02\n",
            "  6.65250644e-02 -1.19686007e-01 -1.91618159e-01 -2.78680027e-02\n",
            " -3.40577774e-02  1.58066407e-01 -9.98528525e-02  1.00832388e-01\n",
            " -1.07066594e-01 -1.43174008e-01 -3.51417989e-01 -7.94541985e-02\n",
            " -2.05754802e-01  1.61233991e-01  2.23230600e-01 -1.57133609e-01\n",
            " -1.37241393e-01 -9.45754051e-02  7.03326017e-02 -1.10211395e-01\n",
            "  2.54891999e-02 -1.34796396e-01 -1.18255969e-02  1.04845405e-01\n",
            "  2.00784206e-01  2.21089602e-01  1.09472811e-01  6.21512011e-02\n",
            "  5.05140051e-02  1.12929389e-01 -1.00145794e-01  1.65029392e-01\n",
            "  4.48303998e-01  2.43794009e-01 -2.57211983e-01 -1.09679200e-01\n",
            " -1.79029793e-01 -2.77850807e-01 -1.64788812e-01 -6.21779934e-02\n",
            " -4.49673980e-02 -9.71802026e-02  5.12939915e-02  3.10027003e-01\n",
            "  3.25958014e-01 -1.34559991e-02 -7.84690008e-02 -1.00946009e-01\n",
            "  4.02679965e-02  9.85773951e-02  1.01724207e-01 -1.59642190e-01\n",
            "  1.65082783e-01  2.43822008e-01 -9.08036083e-02 -9.16061997e-02\n",
            " -2.61426002e-01  1.20557688e-01  8.57044011e-02  4.00398038e-02\n",
            "  6.23788014e-02  4.67919968e-02 -1.45314202e-01  2.83269994e-02]\n"
          ]
        }
      ],
      "source": [
        "# neat little side note - the larger model also has GloVe vectors attached to it\n",
        "# very easy to make use of these in your pipe\n",
        "print(doc_expanded.vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxA2fwUfEccM"
      },
      "source": [
        "## Model training in spaCy\n",
        "The basic model we pulled in has an NER pipeline, but it has a very specific set of entity types.  We could train that model to recognize new types or we could start from scratch and train.  \n",
        "\n",
        "Here we're going to start from scratch - we'll used the project CLI and show a bit of the code under the hood (which I find helpful for debugging)\n",
        "\n",
        "The tutorial we downloaded above has data from Reddit that contains drug names.  We can process that using the included preprocessing script (`ner_drugs/scripts/preprocess.py`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17qgXPDaJn__",
        "outputId": "7279796d-40eb-48f7-de1d-d97cbc387b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "\u001b[38;5;4mℹ Skipping 'preprocess': nothing changed\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!cd ner_drugs && spacy project run preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtwmpFe7KUHk"
      },
      "source": [
        "Now we have preprocessed datasets for training and evaluation.  Let's bring in the training set and walk through how to train our model.\n",
        "\n",
        "SpaCy provides a helper function for training, which is much easier to use than baking your own.  If you want an example of a custom training function, you can see one [here](https://github.com/bpben/ner_chinese_spacy/blob/master/ner_english_example.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxa53bPINw44",
        "outputId": "d9daed5d-b0ce-4928-ecd3-45b5825f883e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: example_model\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     19.33    0.23    0.20    0.28    0.00\n",
            "  0     200         11.60  16541.60    0.00    0.00    0.00    0.00\n",
            "  0     400         19.25   1442.36    0.00    0.00    0.00    0.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "example_model/model-last\n"
          ]
        }
      ],
      "source": [
        "from spacy.cli.train import train\n",
        "\n",
        "# can override config info with overrides\n",
        "# the tutorial config file doesn't have the paths for train/dev corpora\n",
        "# going to just run this for a few epochs, see how it works\n",
        "train(\"./ner_drugs/configs/config.cfg\",\n",
        "      output_path='example_model',\n",
        "      overrides={\"paths.train\": \"./ner_drugs/corpus/drugs_training.spacy\", \n",
        "                 \"paths.dev\": \"./ner_drugs/corpus/drugs_eval.spacy\",\n",
        "                 \"training.max_epochs\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8lJRTEDQwgN"
      },
      "outputs": [],
      "source": [
        "# now we can load our trained model\n",
        "trained_nlp = spacy.load('./example_model/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLGsgsX4Mqcn",
        "outputId": "546065a9-3ded-4a96-9049-4a26714ba67d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Got it out the mud like a pint of lean Always blowing gas, sip,\n",
              " Promethazine,\n",
              " Plays off the hood, bitch I do my thing Got it out the mud like a pint of lean Smoking purple green, sippin' purple lean)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's load in the eval dataset\n",
        "# docbin requires some special handling\n",
        "eval_data = spacy.tokens.DocBin()\n",
        "eval_data = eval_data.from_disk(\"./ner_drugs/corpus/drugs_eval.spacy\")\n",
        "# you can recover the doc objects from the DocBin this way\n",
        "docs = [x for x in eval_data.get_docs(trained_nlp.vocab)]\n",
        "# by running the doc through the model, we get entities out\n",
        "trained_nlp(docs[0]).ents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoVHnaS3lMDm",
        "outputId": "c4bf9e73-000d-4278-96b4-4f18ccbeaff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ents_f': 0.002320185614849188,\n",
              " 'ents_p': 0.002,\n",
              " 'ents_per_type': {'DRUG': {'f': 0.002320185614849188,\n",
              "   'p': 0.002,\n",
              "   'r': 0.0027624309392265192}},\n",
              " 'ents_r': 0.0027624309392265192,\n",
              " 'speed': 29112.548580084105,\n",
              " 'token_acc': 0.9999332071690973,\n",
              " 'token_f': 0.9993542784618469,\n",
              " 'token_p': 0.9991985395609778,\n",
              " 'token_r': 0.9995100659184037}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# spacy models have built-in evaluation functions, but expect Example objects, not Doc\n",
        "corpus = spacy.training.Corpus(\"./ner_drugs/corpus/drugs_eval.spacy\")\n",
        "eval_corpus = list(corpus(trained_nlp))\n",
        "trained_nlp.evaluate(eval_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osP36hp5sxp8"
      },
      "source": [
        "You can see the model does pretty poorly.  Likely this is because it was trained for one epoch.  If we train the model longer, we're likely to see better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO1A6n4aSdOj",
        "outputId": "7c6deeea-79f7-47a9-fce7-9aa5b1ad92d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "\u001b[38;5;4mℹ Skipping 'train': nothing changed\u001b[0m\n",
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "\u001b[38;5;4mℹ Skipping 'evaluate': nothing changed\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# the project cli can do all the work above for us\n",
        "!cd ner_drugs && spacy project run train\n",
        "!cd ner_drugs && spacy project run evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gb-g1MiKSee"
      },
      "source": [
        "Nice! But can we do...better? Preferably with a giant model that everyone likes and is named after Sesame Street.\n",
        "\n",
        "### Spacy-transformers\n",
        "[Spacy-transformers](https://explosion.ai/blog/spacy-transformers) is essentially just a wrapper for [HuggingFace's models](https://huggingface.co/), but made to work with spaCy.\n",
        "\n",
        "Let's take a quick look at how you might be able to use this, generally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjxDWlcQnny7",
        "outputId": "0f7af778-e882-4b21-b2a1-54b22618a284"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from spacy.lang.en import English\n",
        "# minimal example - initialize English model, add in our BioClinicalBERT\n",
        "en = English()\n",
        "# using a custom config - uses BioClinicalBERT\n",
        "# this is Tok2VecTransformer, which combines Transformer+Listener, we'll use something different in training\n",
        "config = {\n",
        "    \"model\": {\n",
        "        \"@architectures\": \"spacy-transformers.Tok2VecTransformer.v3\",\n",
        "        \"name\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
        "        \"tokenizer_config\": {\"use_fast\": True},\n",
        "        # these have to do with alignment\n",
        "        'get_spans': {'@span_getters': 'spacy-transformers.strided_spans.v1',\n",
        "          'stride': 96,\n",
        "          'window': 128},\n",
        "        \"pooling\": {\"@layers\":\"reduce_mean.v1\"} \n",
        "    }\n",
        "}\n",
        "trf = en.add_pipe(\"tok2vec\", config=config)\n",
        "# need to initialize pipeline components\n",
        "en.initialize()\n",
        "# two different contexts\n",
        "ex1 = 'Flintstones vitamins'\n",
        "ex2 = 'Flintstones cartoon'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaxKbZrdKG7",
        "outputId": "5599fe2e-9f46-4a61-bb77-07f2504fff6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768,)\n",
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "# compare to medium model\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "trf_ex1 = en(ex1)\n",
        "trf_ex2 = en(ex2)\n",
        "md_ex1 = nlp(ex1)\n",
        "md_ex2 = nlp(ex2)\n",
        "# BERT \"token vector\" is larger than GloVe\n",
        "print(trf_ex1.vector.shape)\n",
        "print(md_ex1.vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKxxYmSMqEki",
        "outputId": "9e021897-e607-43b6-ebf3-11c3b53f1263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word vector-based similarity \n",
            " [[1.0000002 1.0000002]\n",
            " [1.0000002 1.0000002]]\n",
            "word vector-based similarity \n",
            " [[1.0000002 0.9413736]\n",
            " [0.9413736 1.       ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# we know \"Flintstones\" in ex1 and ex2 are different - but word vectors don't\n",
        "print('word vector-based similarity \\n', \n",
        "      cosine_similarity([md_ex1[0].vector, md_ex2[0].vector]))\n",
        "# the power of transformers! \n",
        "print('word vector-based similarity \\n', \n",
        "      cosine_similarity([trf_ex1[0].vector, trf_ex2[0].vector]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aujlNaRqVXS",
        "outputId": "2f5c9f36-f860-4457-bf74-684d56979703"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(1, 7, 768), (1, 768)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# you CAN play with transformer itself, it's just less friendly for this kind of playing around\n",
        "en = English()\n",
        "# using a custom config - uses BioClinicalBERT\n",
        "# NOTE: definitely check and understand your defaults (spacy recommends it!)\n",
        "config = {\n",
        "    \"model\": {\n",
        "        \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n",
        "        \"name\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
        "        \"tokenizer_config\": {\"use_fast\": True}\n",
        "    }\n",
        "}\n",
        "en.add_pipe(\"transformer\", config=config)\n",
        "en.initialize()\n",
        "ex1 = en('Flintstones vitamins')\n",
        "# comes from ModelOutput - last hidden state and pooled output\n",
        "[x.shape for x in ex1._.trf_data.tensors]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCAeLAtQSbta"
      },
      "source": [
        "Note - a lot of the above is just to mess around in code.  The CLI is much more friendly.  You can assemble a complete pipeline that spacy can load from a config file with `spacy assemble`\n",
        "\n",
        "### Using transformers in training\n",
        "\n",
        "Let's try and bring in a transformer model for training.  You'll need to replace `project.yml` in `ner_drugs` with the modified version available in the [tutorial github repo](https://github.com/bpben/spacy_ner_tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5kes8Z7KRK7",
        "outputId": "8f620899-1485-44ce-a1cc-bc1f25fee1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "================================= train_trf =================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/config_trf.cfg --output training_trf/ --paths.train corpus/drugs_training.spacy --paths.dev corpus/drugs_eval.spacy --gpu-id 0\n",
            "\u001b[38;5;2m✔ Created output directory: training_trf\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training_trf\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-04-14 13:15:16,181] [INFO] Set up nlp object from config\n",
            "[2022-04-14 13:15:16,193] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-04-14 13:15:16,197] [INFO] Created vocabulary\n",
            "[2022-04-14 13:15:16,199] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-04-14 13:15:33,471] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0          18.83     11.08    0.47    0.26    2.21    0.00\n",
            "  0     200        3575.07   1230.93   65.40   60.42   71.27    0.65\n",
            "  0     400          75.84    142.18   70.63   81.29   62.43    0.71\n",
            "  0     600          78.19    142.37   73.00   78.85   67.96    0.73\n",
            "  0     800          58.91    118.63   67.76   59.02   79.56    0.68\n",
            "  0    1000         100.70    161.81   74.02   77.98   70.44    0.74\n",
            "  0    1200          92.36    139.68   72.56   70.39   74.86    0.73\n",
            "  1    1400          86.21    135.17   76.32   76.97   75.69    0.76\n",
            "  1    1600          84.67     99.38   76.43   77.18   75.69    0.76\n",
            "  1    1800          43.59     59.94   73.51   69.36   78.18    0.74\n",
            "  1    2000          71.36    111.49   74.07   66.96   82.87    0.74\n",
            "  1    2200          60.64     68.32   78.63   81.18   76.24    0.79\n",
            "  1    2400         741.71    198.84   78.99   85.76   73.20    0.79\n",
            "  1    2600        4841.10    288.10   77.48   75.26   79.83    0.77\n",
            "  2    2800          90.49    101.95   79.89   83.23   76.80    0.80\n",
            "  2    3000          77.79     99.95   76.13   75.20   77.07    0.76\n",
            "  2    3200         103.85    125.57   77.62   77.62   77.62    0.78\n",
            "  2    3400         122.98     93.20   75.93   71.08   81.49    0.76\n",
            "  2    3600          78.87     74.41   79.61   81.09   78.18    0.80\n",
            "  3    3800          52.65     54.15   77.96   75.92   80.11    0.78\n",
            "  3    4000          75.97     67.20   77.01   75.87   78.18    0.77\n",
            "  3    4200          65.49     80.66   78.96   78.11   79.83    0.79\n",
            "  3    4400          71.06     58.19   76.99   80.85   73.48    0.77\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training_trf/model-last\n",
            "\u001b[1m\n",
            "================================ evaluate_trf ================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate training_trf/model-best corpus/drugs_eval.spacy --output training_trf/metrics.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     99.99\n",
            "NER P   83.23\n",
            "NER R   76.80\n",
            "NER F   79.89\n",
            "SPEED   4170 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "           P       R       F\n",
            "DRUG   83.23   76.80   79.89\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to training_trf/metrics.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# let's do this again, with the modified project and config\n",
        "# made a special set of commands for trf training on GPU\n",
        "!cd ner_drugs && spacy project run train_trf\n",
        "!cd ner_drugs && spacy project run evaluate_trf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Pb6xJEtjVl"
      },
      "source": [
        "Compared to the CNN model, you should see better performance. On this run we're seeing ~7% improvement on F1.   \n",
        "\n",
        "But this is just with one entity type.  In the real world, we're likely to have multiple.  Let's switch to a clinical context and try this with multiple types.\n",
        "\n",
        "### n2c2 2018 Medication Identification Challenge\n",
        "More information in the slides, but below we do a similar approach as above.  We start with a base spaCy model for identifying medications and their attributes.  Then we up the ante with a transformer model.  The results speak for themselves!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zssrnx_U5Xf8"
      },
      "outputs": [],
      "source": [
        "#!unzip n2c2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-MHd1H65pHJ",
        "outputId": "f025ff13-c99b-4dbd-90e0-39820aa33247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/preprocess_i2b2.py raw/training_20180910/ corpus/train.spacy\n",
            "Processed 303 documents: train.spacy\n",
            "Running command: /usr/bin/python3 scripts/preprocess_i2b2.py raw/test corpus/test.spacy\n",
            "Processed 202 documents: test.spacy\n"
          ]
        }
      ],
      "source": [
        "!cd n2c2/ && spacy project run preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PhnH_CHoSbz",
        "outputId": "c94958ef-cd3e-4296-9bd4-dc147337b400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/config.cfg --output training/ --paths.train corpus/train.spacy --paths.dev corpus/test.spacy --paths.vectors en_core_web_md --gpu-id 0\n",
            "\u001b[38;5;2m✔ Created output directory: training\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-04-17 02:09:52,651] [INFO] Set up nlp object from config\n",
            "[2022-04-17 02:09:52,662] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-04-17 02:09:52,666] [INFO] Created vocabulary\n",
            "[2022-04-17 02:09:54,761] [INFO] Added vectors: en_core_web_md\n",
            "[2022-04-17 02:09:55,144] [INFO] Finished initializing nlp object\n",
            "[2022-04-17 02:10:03,358] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    160.94    0.00    0.00    0.00    0.00\n",
            "  2     200        233.14  245884.38    0.00    0.00    0.00    0.00\n",
            "  5     400       4607.60  39027.68    1.32   47.81    0.67    0.01\n",
            "  8     600       7502.04  23547.78    3.62   10.41    2.19    0.04\n",
            " 11     800       5166.81  15835.89   42.75   57.08   34.17    0.43\n",
            " 14    1000      12385.80  15005.81   46.09   58.40   38.06    0.46\n",
            " 17    1200        715.79  11450.01   55.34   64.82   48.27    0.55\n",
            " 20    1400        655.11   8558.53   60.70   68.11   54.74    0.61\n",
            " 23    1600        749.84   7906.45   62.07   68.91   56.48    0.62\n",
            " 26    1800      36354.46   7494.26   64.74   69.75   60.40    0.65\n",
            " 29    2000        890.82   6578.50   66.93   72.69   62.02    0.67\n",
            " 32    2200       1357.06   5625.98   68.15   74.36   62.90    0.68\n",
            " 35    2400      11792.09   5607.49   70.23   75.24   65.84    0.70\n",
            " 38    2600        997.01   4516.76   71.46   76.67   66.92    0.71\n",
            " 41    2800        945.52   4345.90   72.36   77.13   68.14    0.72\n",
            " 44    3000      45455.57   4306.24   73.68   78.64   69.30    0.74\n",
            " 47    3200      21820.95   3712.08   74.29   79.97   69.36    0.74\n",
            " 50    3400       3359.63   3357.55   75.84   79.96   72.12    0.76\n",
            " 52    3600      10624.23   3109.14   76.62   80.91   72.77    0.77\n",
            " 55    3800       3157.90   2822.36   77.00   81.16   73.24    0.77\n",
            " 58    4000       1289.80   2574.73   77.65   81.14   74.45    0.78\n",
            " 61    4200        610.14   2302.53   77.92   82.35   73.95    0.78\n",
            " 64    4400       4061.50   2322.05   78.08   82.30   74.28    0.78\n",
            " 67    4600        658.70   2209.19   78.28   82.20   74.72    0.78\n",
            " 70    4800       3686.99   2016.54   78.33   82.81   74.32    0.78\n",
            " 73    5000        622.53   1921.42   78.59   83.21   74.46    0.79\n",
            " 76    5200        841.07   1855.62   78.80   82.43   75.47    0.79\n",
            " 79    5400        578.64   1604.39   78.92   82.86   75.33    0.79\n",
            " 82    5600        586.15   1654.61   79.10   83.45   75.18    0.79\n",
            " 85    5800        656.32   1546.39   78.93   83.42   74.89    0.79\n",
            " 88    6000        563.81   1462.11   79.10   82.51   75.96    0.79\n",
            " 91    6200        802.64   1450.83   79.17   82.95   75.71    0.79\n",
            " 94    6400       4157.53   1361.72   79.18   82.73   75.93    0.79\n",
            " 97    6600        557.88   1372.87   79.22   83.03   75.74    0.79\n",
            "100    6800        537.24   1265.83   79.31   83.62   75.41    0.79\n",
            "102    7000        530.84   1248.27   79.63   83.69   75.95    0.80\n",
            "105    7200        591.12   1217.92   79.51   83.54   75.85    0.80\n",
            "108    7400        535.48   1120.95   79.71   83.54   76.21    0.80\n",
            "111    7600        520.96   1126.04   79.60   83.72   75.86    0.80\n",
            "114    7800        598.06   1046.34   79.63   83.71   75.93    0.80\n",
            "117    8000        584.19   1138.13   80.02   83.35   76.95    0.80\n",
            "120    8200        509.41   1006.63   80.07   83.55   76.87    0.80\n",
            "123    8400        540.86    987.48   79.98   83.69   76.58    0.80\n",
            "126    8600        498.54    966.03   80.02   83.62   76.71    0.80\n",
            "129    8800        638.61    991.80   80.20   83.35   77.28    0.80\n",
            "132    9000        438.54    885.86   80.24   83.36   77.35    0.80\n",
            "135    9200        745.65    909.66   80.23   83.66   77.07    0.80\n",
            "138    9400        502.65    882.50   80.10   83.79   76.73    0.80\n",
            "141    9600        668.60    929.61   80.17   84.42   76.33    0.80\n",
            "144    9800        470.07    874.41   80.14   83.59   76.97    0.80\n",
            "147   10000        538.19    846.72   80.15   83.49   77.07    0.80\n",
            "150   10200        485.39    834.34   80.17   84.11   76.58    0.80\n",
            "152   10400        464.91    778.89   80.29   84.55   76.44    0.80\n",
            "155   10600        506.67    812.53   80.24   83.95   76.85    0.80\n",
            "158   10800        407.10    709.03   80.19   84.26   76.49    0.80\n",
            "161   11000        610.41    775.94   80.16   84.16   76.53    0.80\n",
            "164   11200        614.71    778.92   80.33   83.91   77.05    0.80\n",
            "167   11400        482.19    729.54   80.36   84.16   76.89    0.80\n",
            "170   11600        460.43    688.89   80.36   84.27   76.79    0.80\n",
            "173   11800        536.24    752.32   80.44   84.00   77.17    0.80\n",
            "176   12000        440.91    712.39   80.56   84.28   77.15    0.81\n",
            "179   12200        537.38    692.25   80.60   84.03   77.43    0.81\n",
            "182   12400        457.76    674.63   80.57   83.93   77.47    0.81\n",
            "185   12600        470.21    707.25   80.54   84.24   77.14    0.81\n",
            "188   12800        456.81    678.15   80.55   84.29   77.14    0.81\n",
            "191   13000        494.27    706.61   80.60   84.29   77.21    0.81\n",
            "194   13200        459.42    650.93   80.62   84.38   77.18    0.81\n",
            "197   13400       4171.91    685.75   80.60   84.24   77.27    0.81\n",
            "200   13600        443.01    639.85   80.68   84.32   77.34    0.81\n",
            "202   13800        420.92    627.51   80.65   84.22   77.37    0.81\n",
            "205   14000       1221.24    744.80   80.65   84.01   77.54    0.81\n",
            "208   14200        610.83    653.01   80.64   84.27   77.31    0.81\n",
            "211   14400       1466.83    653.58   80.63   84.37   77.21    0.81\n",
            "214   14600        566.82    691.04   80.66   84.33   77.29    0.81\n",
            "217   14800       4294.43    631.32   80.66   84.33   77.30    0.81\n",
            "220   15000        439.24    608.05   80.69   84.30   77.38    0.81\n",
            "223   15200        509.42    640.72   80.62   84.28   77.27    0.81\n",
            "226   15400       1189.12    611.65   80.72   84.09   77.61    0.81\n",
            "229   15600        436.54    595.32   80.81   84.37   77.53    0.81\n",
            "232   15800        445.17    626.85   80.68   84.26   77.39    0.81\n",
            "235   16000        428.00    579.81   80.78   84.34   77.50    0.81\n",
            "238   16200        461.98    580.86   80.78   84.56   77.33    0.81\n",
            "241   16400        461.89    588.26   80.79   84.49   77.41    0.81\n",
            "244   16600        523.72    588.41   80.73   84.37   77.39    0.81\n",
            "247   16800        409.86    578.97   80.75   84.40   77.40    0.81\n",
            "250   17000        434.08    576.12   80.72   84.62   77.17    0.81\n",
            "252   17200        433.16    562.70   80.83   84.47   77.48    0.81\n",
            "255   17400        390.73    543.62   80.75   84.24   77.53    0.81\n",
            "258   17600        439.59    555.25   80.77   84.15   77.65    0.81\n",
            "261   17800        468.53    588.35   80.80   84.29   77.58    0.81\n",
            "264   18000        468.83    546.17   80.88   84.36   77.68    0.81\n",
            "267   18200        478.49    603.72   80.81   84.35   77.55    0.81\n",
            "270   18400        424.15    548.15   80.81   84.30   77.59    0.81\n",
            "273   18600        472.78    596.72   80.79   84.24   77.61    0.81\n",
            "276   18800        442.27    557.47   80.84   84.39   77.57    0.81\n",
            "279   19000        828.86    602.23   80.81   84.28   77.61    0.81\n",
            "282   19200        611.76    629.83   80.85   84.47   77.53    0.81\n",
            "285   19400        408.06    512.56   80.80   84.33   77.55    0.81\n",
            "288   19600        639.17    560.75   80.81   84.43   77.49    0.81\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/model-last\n",
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate training/model-best corpus/test.spacy --output training/metrics.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   84.36 \n",
            "NER R   77.68 \n",
            "NER F   80.88 \n",
            "SPEED   76687 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                P       R       F\n",
            "Drug        83.62   76.03   79.64\n",
            "Form        88.69   82.96   85.73\n",
            "Dosage      89.38   78.69   83.69\n",
            "Frequency   78.60   75.61   77.08\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to training/metrics.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!cd n2c2/ && spacy project run train\n",
        "!cd n2c2/ && spacy project run evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeR-s7db4lq_"
      },
      "source": [
        "Okay, not bad, but what if we try with the transformer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8tI7_RduSms",
        "outputId": "c8939a53-2c56-4ee5-92b6-18d1b719bf82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "================================= train_trf =================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/config_trf.cfg --output training_trf/ --paths.train corpus/train.spacy --paths.dev corpus/test.spacy --gpu-id 0\n",
            "\u001b[38;5;2m✔ Created output directory: training_trf\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training_trf\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-04-17 03:37:18,232] [INFO] Set up nlp object from config\n",
            "[2022-04-17 03:37:18,249] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-04-17 03:37:18,254] [INFO] Created vocabulary\n",
            "[2022-04-17 03:37:18,257] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-04-17 03:37:29,538] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         247.15    420.89    0.63    0.33    8.69    0.01\n",
            "  2     200      134387.09  91530.20   73.21   74.73   71.76    0.73\n",
            "  5     400        2460.70  37131.69   85.35   86.62   84.11    0.85\n",
            "  8     600         799.37  35183.99   86.52   85.92   87.12    0.87\n",
            " 11     800         311.76  34664.19   86.57   85.35   87.83    0.87\n",
            " 14    1000         259.59  33911.57   86.84   85.81   87.90    0.87\n",
            " 17    1200         184.08  33133.56   87.09   85.31   88.93    0.87\n",
            " 20    1400         162.19  33749.84   87.01   85.41   88.68    0.87\n",
            " 23    1600       17768.60  31713.80   87.11   87.86   86.38    0.87\n",
            " 26    1800          63.96  32610.86   86.22   85.51   86.95    0.86\n",
            " 29    2000          63.46  30224.07   86.95   85.99   87.94    0.87\n",
            " 32    2200          39.19  29421.30   87.39   87.95   86.83    0.87\n",
            " 35    2400          56.74  28721.71   87.64   88.20   87.08    0.88\n",
            " 38    2600          25.62  26352.55   86.91   86.07   87.76    0.87\n",
            " 41    2800          41.09  24548.17   87.01   86.76   87.26    0.87\n",
            " 44    3000          47.92  21966.79   87.16   87.84   86.49    0.87\n",
            " 47    3200         477.84  19261.38   87.60   88.75   86.48    0.88\n",
            " 50    3400         187.92  16298.01   87.54   87.33   87.76    0.88\n",
            " 52    3600         130.87  13173.53   87.52   87.94   87.11    0.88\n",
            " 55    3800         118.71  10038.14   87.68   87.91   87.44    0.88\n",
            " 58    4000         192.90   7315.91   87.65   88.38   86.94    0.88\n",
            " 61    4200         154.67   5044.29   87.70   88.16   87.23    0.88\n",
            " 64    4400          58.34   3159.93   87.48   87.08   87.88    0.87\n",
            " 67    4600         121.43   1997.94   87.68   88.68   86.69    0.88\n",
            " 70    4800         250.83   1190.18   87.94   88.37   87.50    0.88\n",
            " 73    5000          83.52    674.59   87.68   87.31   88.05    0.88\n",
            " 76    5200        1846.08    528.75   87.08   86.31   87.87    0.87\n",
            " 79    5400         109.21    231.64   87.15   87.32   86.99    0.87\n",
            " 82    5600         855.89    177.54   87.57   87.78   87.36    0.88\n",
            " 85    5800         801.10    112.75   87.57   87.47   87.67    0.88\n",
            " 88    6000         950.08     73.66   87.86   88.01   87.70    0.88\n",
            " 91    6200          26.25     17.05   87.19   87.14   87.24    0.87\n",
            " 94    6400         859.10     32.35   87.99   88.85   87.15    0.88\n",
            " 97    6600         122.05     33.19   87.38   86.55   88.23    0.87\n",
            "100    6800         144.69     20.39   87.96   88.44   87.49    0.88\n",
            "102    7000         209.68     31.29   86.96   85.63   88.32    0.87\n",
            "105    7200         799.43     63.75   87.95   87.76   88.15    0.88\n",
            "108    7400      228125.70    630.64   86.45   84.47   88.53    0.86\n",
            "111    7600       59289.95    186.69   86.67   87.77   85.59    0.87\n",
            "114    7800       95647.24    296.52   87.50   88.01   86.99    0.87\n",
            "117    8000        1183.34     68.41   87.23   86.80   87.67    0.87\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training_trf/model-last\n",
            "\u001b[1m\n",
            "================================ evaluate_trf ================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate training_trf/model-best corpus/test.spacy --output training_trf/metrics.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   88.85 \n",
            "NER R   87.15 \n",
            "NER F   87.99 \n",
            "SPEED   9654  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                P       R       F\n",
            "Drug        91.73   90.21   90.97\n",
            "Dosage      88.24   86.40   87.31\n",
            "Form        88.24   87.04   87.64\n",
            "Frequency   82.30   79.77   81.01\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to training_trf/metrics.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!cd n2c2/ && spacy project run train_trf\n",
        "!cd n2c2/ && spacy project run evaluate_trf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DISveZF-PlEK"
      },
      "source": [
        "Again - we're seeing benefits from switching to this transformer model, an average 10% gain across entity types.\n",
        "\n",
        "Is there a way we can see this improvement by actually loading and trying that model?\n",
        "\n",
        "I've packaged the models off screen here, you can get them via git lfs.  The workflow is described [here](https://spacy.io/usage/saving-loading#models-generating).  \n",
        "\n",
        "The cool thing about packaging these models is spaCy writes all your docs for you.    Models end up as packages spacy can load and install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYUytOgNErM7"
      },
      "outputs": [],
      "source": [
        "# just uncompressing so we can look at the files in here\n",
        "#!tar -xvf en_n2c2_cnn-0.0.1.tar.gz\n",
        "#!tar -xvf en_n2c2_trf-0.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple way to get the example models - git LFS\n",
        "#!apt-get install git-lfs\n",
        "# now pull the example models - this will take some time ~800 MB\n",
        "#!cd spacy_ner_tutorial && git lfs pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHqbGXrMEOxH",
        "outputId": "c2dd628c-e6ab-4090-e71d-b3531f4e5745"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spacy_ner_tutorial'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/54)\u001b[K\rremote: Counting objects:   3% (2/54)\u001b[K\rremote: Counting objects:   5% (3/54)\u001b[K\rremote: Counting objects:   7% (4/54)\u001b[K\rremote: Counting objects:   9% (5/54)\u001b[K\rremote: Counting objects:  11% (6/54)\u001b[K\rremote: Counting objects:  12% (7/54)\u001b[K\rremote: Counting objects:  14% (8/54)\u001b[K\rremote: Counting objects:  16% (9/54)\u001b[K\rremote: Counting objects:  18% (10/54)\u001b[K\rremote: Counting objects:  20% (11/54)\u001b[K\rremote: Counting objects:  22% (12/54)\u001b[K\rremote: Counting objects:  24% (13/54)\u001b[K\rremote: Counting objects:  25% (14/54)\u001b[K\rremote: Counting objects:  27% (15/54)\u001b[K\rremote: Counting objects:  29% (16/54)\u001b[K\rremote: Counting objects:  31% (17/54)\u001b[K\rremote: Counting objects:  33% (18/54)\u001b[K\rremote: Counting objects:  35% (19/54)\u001b[K\rremote: Counting objects:  37% (20/54)\u001b[K\rremote: Counting objects:  38% (21/54)\u001b[K\rremote: Counting objects:  40% (22/54)\u001b[K\rremote: Counting objects:  42% (23/54)\u001b[K\rremote: Counting objects:  44% (24/54)\u001b[K\rremote: Counting objects:  46% (25/54)\u001b[K\rremote: Counting objects:  48% (26/54)\u001b[K\rremote: Counting objects:  50% (27/54)\u001b[K\rremote: Counting objects:  51% (28/54)\u001b[K\rremote: Counting objects:  53% (29/54)\u001b[K\rremote: Counting objects:  55% (30/54)\u001b[K\rremote: Counting objects:  57% (31/54)\u001b[K\rremote: Counting objects:  59% (32/54)\u001b[K\rremote: Counting objects:  61% (33/54)\u001b[K\rremote: Counting objects:  62% (34/54)\u001b[K\rremote: Counting objects:  64% (35/54)\u001b[K\rremote: Counting objects:  66% (36/54)\u001b[K\rremote: Counting objects:  68% (37/54)\u001b[K\rremote: Counting objects:  70% (38/54)\u001b[K\rremote: Counting objects:  72% (39/54)\u001b[K\rremote: Counting objects:  74% (40/54)\u001b[K\rremote: Counting objects:  75% (41/54)\u001b[K\rremote: Counting objects:  77% (42/54)\u001b[K\rremote: Counting objects:  79% (43/54)\u001b[K\rremote: Counting objects:  81% (44/54)\u001b[K\rremote: Counting objects:  83% (45/54)\u001b[K\rremote: Counting objects:  85% (46/54)\u001b[K\rremote: Counting objects:  87% (47/54)\u001b[K\rremote: Counting objects:  88% (48/54)\u001b[K\rremote: Counting objects:  90% (49/54)\u001b[K\rremote: Counting objects:  92% (50/54)\u001b[K\rremote: Counting objects:  94% (51/54)\u001b[K\rremote: Counting objects:  96% (52/54)\u001b[K\rremote: Counting objects:  98% (53/54)\u001b[K\rremote: Counting objects: 100% (54/54)\u001b[K\rremote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/34)\u001b[K\rremote: Compressing objects:   5% (2/34)\u001b[K\rremote: Compressing objects:   8% (3/34)\u001b[K\rremote: Compressing objects:  11% (4/34)\u001b[K\rremote: Compressing objects:  14% (5/34)\u001b[K\rremote: Compressing objects:  17% (6/34)\u001b[K\rremote: Compressing objects:  20% (7/34)\u001b[K\rremote: Compressing objects:  23% (8/34)\u001b[K\rremote: Compressing objects:  26% (9/34)\u001b[K\rremote: Compressing objects:  29% (10/34)\u001b[K\rremote: Compressing objects:  32% (11/34)\u001b[K\rremote: Compressing objects:  35% (12/34)\u001b[K\rremote: Compressing objects:  38% (13/34)\u001b[K\rremote: Compressing objects:  41% (14/34)\u001b[K\rremote: Compressing objects:  44% (15/34)\u001b[K\rremote: Compressing objects:  47% (16/34)\u001b[K\rremote: Compressing objects:  50% (17/34)\u001b[K\rremote: Compressing objects:  52% (18/34)\u001b[K\rremote: Compressing objects:  55% (19/34)\u001b[K\rremote: Compressing objects:  58% (20/34)\u001b[K\rremote: Compressing objects:  61% (21/34)\u001b[K\rremote: Compressing objects:  64% (22/34)\u001b[K\rremote: Compressing objects:  67% (23/34)\u001b[K\rremote: Compressing objects:  70% (24/34)\u001b[K\rremote: Compressing objects:  73% (25/34)\u001b[K\rremote: Compressing objects:  76% (26/34)\u001b[K\rremote: Compressing objects:  79% (27/34)\u001b[K\rremote: Compressing objects:  82% (28/34)\u001b[K\rremote: Compressing objects:  85% (29/34)\u001b[K\rremote: Compressing objects:  88% (30/34)\u001b[K\rremote: Compressing objects:  91% (31/34)\u001b[K\rremote: Compressing objects:  94% (32/34)\u001b[K\rremote: Compressing objects:  97% (33/34)\u001b[K\rremote: Compressing objects: 100% (34/34)\u001b[K\rremote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 54 (delta 15), reused 51 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "UU867K3S8_57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "outputId": "85acb0d8-ce20-44ad-80f7-acfdd739c46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./spacy_ner_tutorial/example_models/en_n2c2_cnn-0.0.1.tar.gz\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-n2c2-cnn==0.0.1) (3.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (4.64.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (1.21.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (1.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (0.9.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (1.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (8.0.15)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-n2c2-cnn==0.0.1) (2.0.1)\n",
            "\u001b[33mDEPRECATION: Source distribution is being reinstalled despite an installed package having the same name and version as the installed package. pip 21.2 will remove support for this functionality. A possible replacement is use --force-reinstall. You can find discussion regarding this at https://github.com/pypa/pip/issues/8711.\u001b[0m\n",
            "Building wheels for collected packages: en-n2c2-cnn\n",
            "  Building wheel for en-n2c2-cnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-n2c2-cnn: filename=en_n2c2_cnn-0.0.1-py3-none-any.whl size=35282842 sha256=55f7ff06a4e9f6ba07fb55be1302ddea6d4d40484b0da5c3711e3291b189f134\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/ff/7e/0337e6e905a7c5984005473be3871b09b485fe074e154eb937\n",
            "Successfully built en-n2c2-cnn\n",
            "Installing collected packages: en-n2c2-cnn\n",
            "  Attempting uninstall: en-n2c2-cnn\n",
            "    Found existing installation: en-n2c2-cnn 0.0.1\n",
            "    Uninstalling en-n2c2-cnn-0.0.1:\n",
            "      Successfully uninstalled en-n2c2-cnn-0.0.1\n",
            "Successfully installed en-n2c2-cnn-0.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "en_n2c2_cnn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install the models\n",
        "!pip install spacy_ner_tutorial/example_models/en_n2c2_cnn-0.0.1.tar.gz\n",
        "!pip install spacy_ner_tutorial/example_models/en_n2c2_trf-0.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "2dd276L2RATe"
      },
      "outputs": [],
      "source": [
        "# you may need to restart the runtime at this point\n",
        "import spacy\n",
        "cnn_n2c2 = spacy.load('en_n2c2_cnn')\n",
        "trf_n2c2 = spacy.load('en_n2c2_trf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udQeaSA1VAS1",
        "outputId": "9b4f43ba-f5d0-4067-9485-aa25a6bdf769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(aspirin, 'Drug'), (tablets, 'Form'), (twice daily, 'Frequency')]\n",
            "[(two, 'Dosage'), (aspirin, 'Drug'), (tablets, 'Form'), (twice daily, 'Frequency')]\n"
          ]
        }
      ],
      "source": [
        "inp = 'two aspirin tablets twice daily'\n",
        "print([(e, e.label_) for e in cnn_n2c2(inp).ents])\n",
        "print([(e, e.label_) for e in trf_n2c2(inp).ents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TL_7p2bO4Lf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "spacy_run_ner_drug_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}